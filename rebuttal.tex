\documentclass[11pt, oneside]{article}   	% use "amsart" instead of "article" for AMSLaTeX format
\usepackage{geometry,color}                		% See geometry.pdf to learn the layout options. There are lots.
\geometry{letterpaper}                   		% ... or a4paper or a5paper or ... 
%\geometry{landscape}                		% Activate for rotated page geometry
%\usepackage[parfill]{parskip}    		% Activate to begin paragraphs with an empty line rather than an indent
\usepackage{graphicx}				% Use pdf, png, jpg, or epsÂ§ with pdflatex; use eps in DVI mode
								% TeX will automatically convert eps --> pdf in pdflatex		
\usepackage{amssymb}
\usepackage{parskip}
%\setlength\parindent{0pt}
%SetFonts

%SetFonts


%\title{Brief Article}
%\author{The Author}
%\date{}							% Activate to display a given date or no date

\begin{document}
%\maketitle

The Editor
Quantitative Science Studies

We thank the reviewers for their constructive critiques as well as their time, effort, and professional courtesy in reviewing our manuscript. We have attempted to respond  in a systematic manner both below and in a revision of the manuscript. 

\section{Reviewer 1}

\emph{This study focuses specifically on a potential flaw in a study by Uzzi, Mukherjee, Stringer, \& Jones  (2013).  Uzzi et al. used articles from the WoS database. They developed an indicator of conventionality and innovativeness for each article by focusing on the pairs of references within each article- and the corresponding pairs of journals associated with those pairs of references. Journals were assumed to represent areas of knowledge. Articles that built upon journal-pairs that were commonly mentioned together represented conventionality.  Articles that built upon journals pairs that were rarely mentioned together represented innovativeness. They found that articles with high conventionality and high innovative (HC; HI) were more highly cited (a commonly used indicator that the paper has had a greater impact). Uzzi et al. also went through a variety of sensitivity analyses to show that the results held across different disciplinary fields (they used standard definitions of disciplines).}   

We largely concur but would like to add that since journal pairs are computed across the dataset being studied, frequently occurring journal pairs can be a composite of both frequently and rarely co-cited reference pairs. Secondly, the sensitivity analyses of Uzzi are challenged in Boyack and Klavans, 2014, to wit,``However, their detailed results showed that the N+C+ bin in the 2x2 matrix had the highest probability of containing a hit paper for only 64.4\% of 243 WoS subject categories.'' 

\emph{ This study doesn't question the database. They don't question the assumption that conventionality and innovativeness can be detected by calculating within-article journal co-occurrences. Their point of departure is in the calculation of z-scores (the deviation of journal co-occurences from expected values). Uzzi et al. used a random reference substitution approach to determine z-scores. The authors in this article suggest that substitutions should not be random.  One doesn't substitute a reference to a physics journal with a reference to a social science journal.  Substitutions should be within the local field.  Using their alternative approach, they show that Uzzi et al's central finding (that high conventionality and high innovativeness results in higher impact across many disciplines) does not apply equally across the three broadly defined fields they focused on: applied physics, immunology and metabolism. They found that It doesn't apply equally across WoS disciplinary categories. This is, in essence, a highly focused methodological critique of an article that was published over five years ago.  As such, the technical contribution of this article, while important, may be marginal. The authors seem to suggest that their new substation approach should be used. I'm not convinced.}

The reviewer is correct that we do not question the database--the Web of Science is sufficiently comprehensive to conduct such studies, and is also correct about our statement that  random substitutions should not be made without consideration of  disciplinary context. However,  we certainly do not endorse  the assumption that  ``conventionality  and innovativeness can be detected by calculating journal co-occurrences". Instead, we point out the challenges in using co-citation analysis in assessing these properties. 

We agree that our study is centered around a focused methodological critique. One that has important implications for whether co-citation analysis can be used to infer novelty and conventionality. However, we do not agree that its contribution is marginal merely because Uzzi was published five years ago. By this standard, a critical consideration or extension of Small and Marshakova who formulated the concept of co-citation could also be considered marginal, With the exception of a challenge by Boyack and Klavans (2014), Uzzi seems to have been cited (rather uncritically) over 175 times since its publication placing it in the top 1-2\% of papers and we think that our work has value in identifying a significant weakness inherent in its approach.

\emph{As such, I recommend that the authors resubmit the article after resolving the following issues. 1.  Focus on the issue at hand.  Much of the early text is totally off the point of the study. This is not a study in semantics. Nor is it about co-citation analysis and disciplinary affects. The title seems totally misleading. The article is about a very narrow methodological issue: whether the construction of Z-scores using a random substitution model is inherently biased vis-à-vis construction of Z-scores using a local substitution model.  So if the authors choose to rewrite- please edit out all of the superfluous material. It's a huge distraction.} 

We have revised the text to be considerate of these suggestions, which we value. However, we would like to comment that the methodological issue we elucidate in the paper only has relevance in the larger context of co-citation analysis and differences in citation practice between disciplines. Thus, we are unwilling to discard placing the work in a larger context. We agree that this manuscript is not a study in semantics, indeed we do not make such an assertion. However, the local networks we study are semantically themed. We respectfully disagree that the title is misleading. 

\emph{2.  Realize that the proposed approach is also flawed.  On the surface- it would make sense- don't substitute an anthropology journal for a physics journal. But deeper down, it may not make sense because there is no agreed upon way to decide what is `local' and what isn't.  Does one use word searches to define fields (as suggested by the authors)?  Subject categories?  And since there's so much latitude in what one could use- aren't you running the risk of getting a false negative because of misspecification of the discipline?  Stated differently, the proposed approach may be equally flawed because most highly influential journals (such as Science and Nature) can't be assigned to fields or disciplines.   These highly influential journals were found to have an excessively strong effect on the indicator of conventionality and innovativeness in Uzzi's study (see Boyack \& Klavans, 2014)- so a change in how the Z-score is calculated for this group will have a disproportional effect on the results. It's not clear how does this new approach address this issue.  Science and Nature (and the other high influence journals) can publish across all three of their fields (immunology, applied physics and metabolism)- who should their cohort group be?  If a very small set of journals can have such a huge influence on the outcome- will the local substitution approach really help or hurt?}

The reviewer makes two very valid points, the first being that we should acknowledge that the model we use is also flawed, and the second having to do with challenges in using co-citation analysis defined by journals in characterizing which publications are conventional and/or innovative. In response to the first point, we do not assert that our model/method is perfect -- in fact we acknowledge this explicitly, and have revised the paper to make that point even stronger. But we do note that we establish a reduction in model misspecification when using our model compared to Uzzi et al.'s model (as indicated by a reduction in the Kullback-Leibler divergence), and so even though our model will also be flawed (and it will be), it is less flawed than Uzzi's. With respect to the second point, we have reiterated (in citing Boyack and Klavans, 2014) that there are journal effects that we do not investigate in this study but these are offset slightly by the use of keyword searches to define a network.

A keyword search defines a set of articles without modifying the references cited in those publications. These references were selected by the authors of the articles and while they are typically dominated by the closest discipline, they can be drawn from any discipline (as we also cite in our manuscript: Garfield, 1979; Klavans \& Boyack, 2017; Moed, 2010; Wallace, Lariviere, \& Gingras, 2012). In our approach, the expected values for z-score calculations are calculated from references cited by the authors of those articles in contrast to the indiscriminate substitutions of Uzzi. Thus, Science and Nature are accounted for without the baggage of the articles in them that are irrelevant to the network being studied.
 
\emph{3. Provide more concrete evidence that one approach is better than the other.  For example- use Uzzi's approach and your new approach to nominate the `high innovative' and `high conventional' papers.   Then focus on differences in these sets- the papers that are HI-using the old method vs. HI using the new method; HC-old vs. HC-new method; LI new vs. LI old; LC new vs. LC old).  Rely on a third approach to make the judgment about which approach is better (such as the recent work by Henry Small that detects whether a paper is a discovery paper using citance analysis). I suspect that, if one is trying to identify innovative papers, citance analysis that focuses on discovery (as a surrogate for innovation) is a reasonable approach- and gives a more concrete way of assessing the merits of the two approaches. Since these sets of papers can (and should) be made public- others can also see (for themselves) which method really works best.} 

Again, we thank the reviewer for this interesting suggestion. However, the objective of this study was to demonstrate that a small change in the  Uzzi model results in a reduction in model misspecification and presents very different trends. Hence, any conclusions based on Uzzi's model that are not also seen  using our model are likely to be the result of  model misspecification. (Equally importantly, since the reviewer points out our model is also likely flawed, it does not mean that trends found using our model are valid!) Moreover, we are not trying to identify innovative papers--our point is more that the proxy measure for impact suggests that HC flavored with HI (as defined by Uzzi) is not a universal feature of highly cited papers. Finally, we would argue that a mixed methods approach where qualitative judgment is brought to bear on quantitative results as per (Ioannidis, Klavans, and Boyack) is an excellent suggestion, but we think it's beyond the scope of this manuscript.

\section{Reviewer 2}

\emph{This paper builds on the Uzzi et al (2013) Science paper.  It argues that a local network approach (rather than Uzzi's global network approach) is appropriate for identifying unusual combinations.  It also shows that Uzzi's findings regarding the relationship between novelty X conventionality and citation impact does not hold when using the local network approach. I really enjoyed reading this paper and believe that it is an important contribution: the topic and findings are important, data analysis is thorough, and paper is well-written.}

Thank you. 
 
\emph{However, one major concern is about whether a local network is indeed more appropriate than a global network.  The authors did give very convincing arguments. }

We agree that there are limitations to local networks as well as to global networks, but as our study shows that model misspecification is dramatically reduced by restricting substitutions to the local network, we feel that it is reasonable to claim that local networks are, overall, more appropriate
than global networks.

\emph{However, it can be argued that most atypical combinations are cross-disciplinary.  By using a local network for reference swapping, these cross-disciplinary atypical combinations will not be identified as such.  At the fundamental level, what is the relationship between novelty and interdisciplinary?  Clearly they are not empirically independent of each other, but what about their conceptual relationship?  Should we consider novelty as something net of interdisciplinary, such that a local network is appropriate in order to clear out interdisciplinarity, or should we consider novelty as something that is related with interdisciplinarity, such that a global network is needed for not missing cross-disciplinary unusual combinations.  I would suggest the authors do develop a clear conceptualization of novelty before claiming that local network is appropriate.  In addition, it might be interesting to separate within-discipline and cross-discipline novelty and examine their effects on citation impact.}

We thank the reviewer for these interesting and thought provoking points. To respond to this, we  wish to elaborate on how we define the local networks, and hence which substitutions are allowed. The way we defined the local network is that it contains all papers that are cited by any paper recognized by the keyword search.  Thus, \emph{the local network explicitly includes papers that are from multiple disciplines}, while still being restricted to those papers that are cited. In contrast, if we had defined the local network to papers that citing references from a single discipline, it would have indeed prevented interdisciplinary citations, and most likely have produced a very different outcome. In other words, our approach does not alter the original references cited by the authors of the articles we assemble into datasets (we raise this point in our response to Reviewer 1 as well).

\emph{Figure 2 is informative, it shows that when using local network instead of global network, the Uzzi finding still holds for the whole WoS database but not for applied physics, immunology, or metabolism. However, there are two possible explanations: (1) Uzzi's findings are not universal across fields, and (2) difference in using local vs. global networks.  It would be helpful to replicate the same plots using Uzzi's global network approach and single out which factor made the difference.}

The figure below was included in an earlier version of this manuscript submitted to another journal and was criticized for being confusing. It specifically addresses the reviewer's point. Thus, both factors are in play- there is a difference between local and global. Disciplines can be different from each other. 
 
\emph{On page 2, `In addition, under this random model, a reference cited many times in a given year is selected with the same probability as a reference cited only once, which appears inconsistent with the power law or lognormal citation distributions described in the literature.'  I do not quite understand this.  In Uzzi's reference swapping, for each references, its total number of forward citations is preserved, so at the system level, a reference with fewer citations does have a smaller probability of being swapped.  So I do not exactly understand this criticism.}

Our apologies for not being clear. In examining the citation shuffling code of Uzzi (kindly shared by Satyam Mukherjee and Ben Jones), we realized that the choice of substitutions was restricted to the set rather than the multi-set of eligible references. Their reciprocal substitution approach built-in to their code ensures that the total number of references is still preserved after shuffling but selection is equiprobable. In other words, Uzzi's algorithm systematically shuffles every reference but replacements are selected from the set, e.g., a,b,c,d... rather than the multi-set, e.g., aaaa, b, ccc, dd... of eligible references.

a) Consider a toy dataset slice of 5 publications all published in the same year. Each of these publications cites 4 references, thus, 20 citations. For simplicity, we will assume that all of these references were published in the same year. Of these, one (which we will call $x$) is cited in each of the publications (thus cited 5 times) and the remaining 15 are cited once in total. During citation shuffling (i)  in our approach, the probability of selecting $x$ would be 5/20. In Uzzi it would be 1/15. Further, Uzzi's code enforces 20 successive substitution attempts if the original reference is selected as a substitute. Our code allows such a replacement.

b) In a local or a global network it does not make sense to treat a highly cited reference as equiprobable with a reference cited only once. 

c) We should add a section on Jim's analysis showing 2.5\% of the references are affected- for a distribution in which the top 1-2\% qualify as highly cited then it becomes important.

\end{document}  